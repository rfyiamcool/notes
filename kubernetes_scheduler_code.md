# æºç åˆ†æ kubernetes scheduler æ ¸å¿ƒè°ƒåº¦å™¨çš„å®ç°åŸç†

![](https://xiaorui-cc.oss-cn-hangzhou.aliyuncs.com/images/202301/202301251502086.png)

> åŸºäº kubernetes `v1.27.0` æºç åˆ†æ scheduler è°ƒåº¦å™¨

k8s scheduler çš„ä¸»è¦èŒè´£æ˜¯ä¸ºæ–°åˆ›å»ºçš„ pod å¯»æ‰¾ä¸€ä¸ªæœ€åˆé€‚çš„ node èŠ‚ç‚¹, ç„¶åè¿›è¡Œ bind node ç»‘å®š, åé¢ kubelet æ‰ä¼šç›‘å¬åˆ°å¹¶åˆ›å»ºçœŸæ­£çš„ pod.

é‚£ä¹ˆé—®é¢˜æ¥äº†, å¦‚ä½•ä¸º pod å¯»æ‰¾æœ€åˆé€‚çš„ node ? è°ƒåº¦å™¨éœ€è¦ç»è¿‡ predicates é¢„é€‰å’Œ priority ä¼˜é€‰.

- é¢„é€‰å°±æ˜¯ä»é›†ç¾¤çš„æ‰€æœ‰èŠ‚ç‚¹ä¸­æ ¹æ®è°ƒåº¦ç®—æ³•ç­›é€‰å‡ºæ‰€æœ‰å¯ä»¥è¿è¡Œè¯¥ pod çš„èŠ‚ç‚¹é›†åˆ
- ä¼˜é€‰åˆ™æ˜¯æŒ‰ç…§ç®—æ³•å¯¹é¢„é€‰å‡ºæ¥çš„èŠ‚ç‚¹è¿›è¡Œæ‰“åˆ†ï¼Œæ‰¾åˆ°åˆ†å€¼æœ€é«˜çš„èŠ‚ç‚¹ä½œä¸ºè°ƒåº¦èŠ‚ç‚¹.

é€‰å‡ºæœ€ä¼˜èŠ‚ç‚¹å, å¯¹ apiserver å‘èµ· pod èŠ‚ç‚¹ bind æ“ä½œ, å…¶å®å°±æ˜¯å¯¹ pod çš„ spec.NodeName èµ‹å€¼æœ€ä¼˜èŠ‚ç‚¹.

**æºç åŸºæœ¬è°ƒç”¨å…³ç³»**

![](https://xiaorui-cc.oss-cn-hangzhou.aliyuncs.com/images/202212/k8s-scheduler.jpg)

## k8s scheduler å¯åŠ¨å…¥å£

k8s scheduler åœ¨å¯åŠ¨æ—¶ä¼šè°ƒç”¨åœ¨ cobra æ³¨å†Œçš„ `setup` æ¥åˆå§‹åŒ– scheduler è°ƒåº¦å™¨å¯¹è±¡.

ä»£ç ä½ç½®: `cmd/kube-scheduler/app/server.go`

```go
func Setup(ctx context.Context, opts *options.Options, outOfTreeRegistryOptions ...Option) (*schedulerserverconfig.CompletedConfig, *scheduler.Scheduler, error) {
	// è·å–é»˜è®¤é…ç½®
	if cfg, err := latest.Default(); err != nil {
		return nil, nil, err
	} else {
		opts.ComponentConfig = cfg
	}

	// éªŒè¯ scheduler çš„é…ç½®å‚æ•°
	if errs := opts.Validate(); len(errs) > 0 {
		return nil, nil, utilerrors.NewAggregate(errs)
	}

	c, err := opts.Config()
	if err != nil {
		return nil, nil, err
	}

	// é…ç½®ä¸­å¡«å……å’Œè°ƒæ•´
	cc := c.Complete()

	...

	// æ„å»º scheduler å¯¹è±¡
	sched, err := scheduler.New(cc.Client,
		cc.InformerFactory,
		cc.DynInformerFactory,
		recorderFactory,
		ctx.Done(),
		...
	)
	if err != nil {
		return nil, nil, err
	}
	...

	return &cc, sched, nil
}
```

å®ä¾‹åŒ– kubernetes scheduler å¯¹è±¡, åˆå§‹åŒ–æµç¨‹ç›´æ¥çœ‹ä¸‹é¢ä»£ç .

ä»£ç ä½ç½®: `pkg/scheduler/scheduler.go`

```go
// New returns a Scheduler
func New(client clientset.Interface,
	informerFactory informers.SharedInformerFactory,
	dynInformerFactory dynamicinformer.DynamicSharedInformerFactory,
	recorderFactory profile.RecorderFactory,
	stopCh <-chan struct{},
	opts ...Option) (*Scheduler, error) {


	// æ„å»º registry å¯¹è±¡, é»˜è®¤é›†æˆäº†ä¸€å †çš„æ’ä»¶
	registry := frameworkplugins.NewInTreeRegistry()
	if err := registry.Merge(options.frameworkOutOfTreeRegistry); err != nil {
		return nil, err
	}

	// profiles ç”¨æ¥ä¿å­˜ä¸åŒè°ƒåº¦å™¨çš„ framework æ¡†æ¶, framework åˆ™ç”¨æ¥å­˜æ”¾ plugin.
	profiles, err := profile.NewMap(options.profiles, registry, recorderFactory, stopCh,
		frameworkruntime.WithComponentConfigVersion(options.componentConfigVersion),
		frameworkruntime.WithClientSet(client),
		frameworkruntime.WithKubeConfig(options.kubeConfig),
		frameworkruntime.WithInformerFactory(informerFactory),
		...
		...
	)

	// å®ä¾‹åŒ–å¿«ç…§
	snapshot := internalcache.NewEmptySnapshot()

	// å®ä¾‹åŒ– queue, è¯¥ queue ä¸º PriorityQueue.
	podQueue := internalqueue.NewSchedulingQueue(
		profiles[options.profiles[0].SchedulerName].QueueSortFunc(),
		informerFactory,
		...
	)

	// å®ä¾‹åŒ– cache ç¼“å­˜
	schedulerCache := internalcache.New(durationToExpireAssumedPod, stopEverything)

	// å®ä¾‹åŒ– scheduler å¯¹è±¡
	sched := &Scheduler{
		Cache:                    schedulerCache,
		client:                   client,
		nodeInfoSnapshot:         snapshot,
		NextPod:                  internalqueue.MakeNextPodFunc(podQueue),
		StopEverything:           stopEverything,
		SchedulingQueue:          podQueue,
	}
	sched.applyDefaultHandlers()

	// åœ¨ informer é‡Œæ³¨å†Œè‡ªå®šä¹‰çš„äº‹ä»¶å¤„ç†æ–¹æ³•
	addAllEventHandlers(sched, informerFactory, dynInformerFactory, unionedGVKs(clusterEventMap))

	return sched, nil
}

func (s *Scheduler) applyDefaultHandlers() {
	s.SchedulePod = s.schedulePod
	s.FailureHandler = s.handleSchedulingFailure
}
```

## æ³¨å†Œåœ¨ scheduler informer çš„å›è°ƒæ–¹æ³•

åœ¨ informer é‡Œæ³¨å†Œ pod å’Œ node èµ„æºçš„å›è°ƒæ–¹æ³•ï¼Œç›‘å¬ pod äº‹ä»¶å¯¹ queue å’Œ cache åšå›è°ƒå¤„ç†. ç›‘å¬ node äº‹ä»¶å¯¹ cache åšå¤„ç†.

```go
func addAllEventHandlers(
	sched *Scheduler,
	informerFactory informers.SharedInformerFactory,
	dynInformerFactory dynamicinformer.DynamicSharedInformerFactory,
	gvkMap map[framework.GVK]framework.ActionType,
) {
	// ç›‘å¬ pod äº‹ä»¶ï¼Œå¹¶æ³¨å†Œå¢åˆ æ”¹å›è°ƒæ–¹æ³•, å…¶æ“ä½œæ˜¯å¯¹ cache çš„å¢åˆ æ”¹
	informerFactory.Core().V1().Pods().Informer().AddEventHandler(
		cache.FilteringResourceEventHandler{
			FilterFunc: func(obj interface{}) bool {
				...
			},
			Handler: cache.ResourceEventHandlerFuncs{
				AddFunc:    sched.addPodToCache,
				UpdateFunc: sched.updatePodInCache,
				DeleteFunc: sched.deletePodFromCache,
			},
		},
	)

	// ç›‘å¬ pod äº‹ä»¶ï¼Œå¹¶æ³¨å†Œå¢åˆ æ”¹æ–¹æ³•, å¯¹ queue æ’å…¥å¢åˆ æ”¹äº‹ä»¶.
	informerFactory.Core().V1().Pods().Informer().AddEventHandler(
		cache.FilteringResourceEventHandler{
			FilterFunc: func(obj interface{}) bool {
				...
			},
			Handler: cache.ResourceEventHandlerFuncs{
				AddFunc:    sched.addPodToSchedulingQueue,
				UpdateFunc: sched.updatePodInSchedulingQueue,
				DeleteFunc: sched.deletePodFromSchedulingQueue,
			},
		},
	)

	// ç›‘å¬ node äº‹ä»¶ï¼Œæ³¨å†Œå›è°ƒæ–¹æ³•ï¼Œè¯¥æ–¹æ³•åœ¨ cache é‡Œå¯¹ node çš„å¢åˆ æ”¹æŸ¥.
	informerFactory.Core().V1().Nodes().Informer().AddEventHandler(
		cache.ResourceEventHandlerFuncs{
			AddFunc:    sched.addNodeToCache,
			UpdateFunc: sched.updateNodeInCache,
			DeleteFunc: sched.deleteNodeFromCache,
		},
	)
}
```

## scheduler çš„é€‰ä¸¾å®ç°

`kube-scheduler` è·Ÿ k8s çš„å…¶ä»–ä¸»æ§ç»„ä»¶ä¸€æ ·, ä¹Ÿä¼šé€šè¿‡é€‰ä¸¾ `leaderelection` æœºåˆ¶ä¿è¯é›†ç¾¤åªæœ‰ä¸€ä¸ª leader å®ä¾‹è¿è¡Œè°ƒåº¦å™¨, å…¶ä»– follower å®ä¾‹åˆ™å°è¯•è½®è¯¢æŠ¢é”ç›´åˆ°æˆåŠŸ.

æºç ä½ç½®: `cmd/kube-scheduler/app/server.go`

```go
func Run(ctx context.Context, cc *schedulerserverconfig.CompletedConfig, sched *scheduler.Scheduler) error {
	...

	waitingForLeader := make(chan struct{})
	isLeader := func() bool {
		select {
		case _, ok := <-waitingForLeader:
			// if channel is closed, we are leading
			return !ok
		default:
			// channel is open, we are waiting for a leader
			return false
		}
	}


	// å¯åŠ¨ informers, è¿™é‡Œåªæœ‰ pod å’Œ node.
	cc.InformerFactory.Start(ctx.Done())

	// åŒæ­¥ informer çš„æ•°æ®åˆ°æœ¬åœ°ç¼“å­˜
	cc.InformerFactory.WaitForCacheSync(ctx.Done())

	// å¦‚æœåœ¨é…ç½®ä¸­å¯åŠ¨äº†é€‰ä¸¾, åˆ›å»ºé€‰ä¸¾å¯¹è±¡, æ³¨å†Œäº‹ä»¶æ–¹æ³•, å¹¶å¯ç”¨é€‰ä¸¾.
	if cc.LeaderElection != nil {
		cc.LeaderElection.Callbacks = leaderelection.LeaderCallbacks{
			OnStartedLeading: func(ctx context.Context) {
				// å½“é€‰ä¸¾æ‹¿åˆ° leader æ—¶, å¯åŠ¨ scheduler è°ƒåº¦å™¨
				close(waitingForLeader)
				sched.Run(ctx)
			},
			OnStoppedLeading: func() {
				// å½“é€‰ä¸¾æˆåŠŸä½†åé¢åˆä¸¢å¤± leader å, åˆ™é€€å‡ºè¿›ç¨‹.
				// è¿›ç¨‹é€€å‡ºå, ä¼šè¢« docker æˆ– systemd é‡æ–°æ‹‰èµ·, å°è¯•æ‹¿é”.
				select {
				case <-ctx.Done():
					// We were asked to terminate. Exit 0.
					os.Exit(0)
				default:
					// We lost the lock.
					klog.ErrorS(nil, "Leaderelection lost")
					klog.FlushAndExit(klog.ExitFlushTimeout, 1)
				}
			},
		}

		// æ„å»º leaderelection å¯¹è±¡
		leaderElector, err := leaderelection.NewLeaderElector(*cc.LeaderElection)
		if err != nil {
			return fmt.Errorf("couldn't create leader elector: %v", err)
		}

		// å¯åŠ¨é€‰ä¸¾
		leaderElector.Run(ctx)

		return fmt.Errorf("lost lease")
	}

	// å¦‚æœæ²¡æœ‰å¼€å¯é€‰ä¸¾, åˆ™ç›´æ¥å¯åŠ¨ scheduler è°ƒåº¦å™¨.
	close(waitingForLeader)
	sched.Run(ctx)
	return fmt.Errorf("finished without leader elect")
}
```

å…³äº client-go LeaderElection é€‰ä¸¾çš„å®ç°åŸç†, è¯·ç‚¹å‡»ä¸‹é¢è¿æ¥. 

[https://github.com/rfyiamcool/notes/blob/main/kubernetes_leader_election_code.md](https://github.com/rfyiamcool/notes/blob/main/kubernetes_leader_election_code.md)

## scheudler å¯åŠ¨å…¥å£

`Run()` æ–¹æ³•æ˜¯ k8s scheduler çš„å¯åŠ¨è¿è¡Œå…¥å£, å…¶æµç¨‹æ˜¯å…ˆå¯åŠ¨ queue é˜Ÿåˆ—çš„ Run æ–¹æ³•, å†å¼‚æ­¥å¯åŠ¨ä¸€ä¸ªåç¨‹å¤„ç†æ ¸å¿ƒè°ƒåº¦æ–¹æ³• `scheduleOne`.

`schedulingQueue` çš„ `Run()` æ–¹æ³•ç”¨æ¥ç›‘å¬å†…éƒ¨çš„å»¶è¿Ÿä»»åŠ¡, æŠŠåˆ°æœŸçš„ä»»åŠ¡æ”¾åˆ° activeQ ä¸­.

è€Œ `scheduleOne` æ–¹æ³•ç”¨æ¥ä»ä¼˜å…ˆçº§é˜Ÿåˆ—é‡Œè·å–ç”± informer æ’å…¥çš„ pod å¯¹è±¡, è°ƒç”¨ `schedulingCycle` ä¸º pod é€‰æ‹©æœ€ä¼˜çš„ node èŠ‚ç‚¹. å¦‚æœæ‰¾åˆ°äº†åˆé€‚çš„ node èŠ‚ç‚¹, åˆ™è°ƒç”¨ `bindingCycle` æ–¹æ³•æ¥å‘èµ· pod å’Œ node ç»‘å®š.

æºç ä½ç½®: `pkg/scheduler/scheduler.go`

```go
func (sched *Scheduler) Run(ctx context.Context) {
	sched.SchedulingQueue.Run()

	go wait.UntilWithContext(ctx, sched.scheduleOne, 0)

	<-ctx.Done()
	sched.SchedulingQueue.Close()
}

func (sched *Scheduler) scheduleOne(ctx context.Context) {
	// ä» activeQ ä¸­è·å–éœ€è¦è°ƒåº¦çš„ pod æ•°æ®
	podInfo := sched.NextPod()
	pod := podInfo.Pod

	// ä¸º pod é€‰æ‹©æœ€ä¼˜çš„ node èŠ‚ç‚¹
	scheduleResult, assumedPodInfo, err := sched.schedulingCycle(schedulingCycleCtx, state, fwk, podInfo, start, podsToActivate)
	if err != nil {
		// å¦‚ä½•æ²¡æœ‰æ‰¾åˆ°èŠ‚ç‚¹ï¼Œåˆ™æ‰§è¡Œå¤±è´¥æ–¹æ³•.
		sched.FailureHandler(schedulingCycleCtx, fwk, assumedPodInfo, err, scheduleResult.reason, scheduleResult.nominatingInfo, start)
		return
	}

	go func() {
		// åƒ apiserver å‘èµ· pod -> node ç»‘å®š
		status := sched.bindingCycle(bindingCycleCtx, state, fwk, scheduleResult, assumedPodInfo, start, podsToActivate)
		if !status.IsSuccess() {
			sched.handleBindingCycleError(bindingCycleCtx, state, fwk, assumedPodInfo, start, scheduleResult, status)
		}
	}()
}
```

`NextPod` åº•å±‚å¼•ç”¨äº† `MakeNextPodFunc` æ–¹æ³•, å…¶å†…éƒ¨ä» `PriorityQueue` é˜Ÿåˆ—ä¸­è·å– pod å¯¹è±¡.

```go
func MakeNextPodFunc(queue SchedulingQueue) func() *framework.QueuedPodInfo {
	return func() *framework.QueuedPodInfo {
		podInfo, err := queue.Pop()
		if err == nil {
			return podInfo
		}
		return nil
	}
}

func (p *PriorityQueue) Pop() (*framework.QueuedPodInfo, error) {
	p.lock.Lock()
	defer p.lock.Unlock()

	for p.activeQ.Len() == 0 {
		// æ²¡æœ‰æ•°æ®åˆ™é™·å…¥æ¡ä»¶å˜é‡çš„ç­‰å¾…æ¥å£
		p.cond.Wait()
	}
	obj, err := p.activeQ.Pop()
	if err != nil {
		return nil, err
	}
	pInfo := obj.(*framework.QueuedPodInfo)
	pInfo.Attempts++  // æ¯æ¬¡ pop åéƒ½å¢åŠ ä¸‹ attempts æ¬¡æ•°.
	return pInfo, nil
}
```

## å•å®ä¾‹å•åç¨‹æ¨¡å‹çš„ schduler è°ƒåº¦å™¨

![](https://xiaorui-cc.oss-cn-hangzhou.aliyuncs.com/images/202301/202301261858892.png)

éœ€è¦å…³æ³¨çš„æ˜¯æ•´ä¸ª kubernetes scheduler è°ƒåº¦å™¨åªæœ‰ä¸€ä¸ªåç¨‹å¤„ç†ä¸»è°ƒåº¦å¾ªç¯ `scheduleOne`, è™½ç„¶ kubernetes scheduler å¯ä»¥å¯åŠ¨å¤šä¸ªå®ä¾‹, ä½†å¯åŠ¨æ—¶éœ€è¦ leaderelection é€‰ä¸¾, åªæœ‰ leader æ‰å¯ä»¥å¤„ç†è°ƒåº¦, å…¶ä»–èŠ‚ç‚¹ä½œä¸º follower ç­‰å¾… leader å¤±æ•ˆ. ä¹Ÿå°±æ˜¯è¯´æ•´ä¸ª k8s é›†ç¾¤è°ƒåº¦æ ¸å¿ƒçš„å¹¶å‘åº¦ä¸º 1 ä¸ª. 

äº‘åŸç”Ÿç¤¾åŒºä¸­æœ‰äººä½¿ç”¨ kubemark æ¨¡æ‹Ÿ 2000 ä¸ªèŠ‚ç‚¹çš„è§„æ¨¡æ¥å‹æµ‹ kube-scheduler å¤„ç†æ€§èƒ½åŠæ—¶å»¶, æµ‹è¯•ç»“æœæ˜¯ 30s å†…å®Œæˆ 15000 ä¸ª pod è°ƒåº¦ä»»åŠ¡. è™½ç„¶ kube-scheduler æ˜¯å•å¹¶å‘æ¨¡å‹, ä½†ç”±äºé¢„é€‰å’Œä¼˜é€‰éƒ½å±äºè®¡ç®—å‹ä»»åŠ¡éé˜»å¡IO, åˆæœ‰ `percentageOfNodesToScore` å‚æ•°ä¼˜åŒ–, æœ€é‡è¦çš„æ˜¯åˆ›å»º pod çš„æ“ä½œé€šå¸¸ä¸ä¼šå¤ªé«˜å¹¶å‘. è¿™å‡ ç‚¹ä¸‹æ¥å•å¹¶å‘æ¨¡å‹çš„ scheduler ä¹Ÿè¿˜å¯ä»¥æ¥å—çš„.

### ä¸ºä»€ä¹ˆ scheduler ä¸æ”¯æŒå¹¶å‘ ?

æŒ‰ç…§å½“å‰ scheudler è°ƒåº¦å™¨çš„è®¾è®¡åŸç†, ä½¿ç”¨é¢„é€‰å’Œä¼˜é€‰ç®—æ³•é€‰å‡ºæœ€åˆé€‚çš„èŠ‚ç‚¹, å¹¶å‘åœºæ™¯ä¸‹æ— æ³•ä¿è¯å®‰å…¨, æ¯”å¦‚, é€‰å‡ºçš„æœ€ä¼˜èŠ‚ç‚¹åœ¨å¹¶å‘ä¸‹ä¼šè¢«å¤šä¸ª pod ç»‘å®š.

### ä½¿ç”¨è‡ªå®šä¹‰è°ƒåº¦å™¨è¿›è¡Œå¹¶å‘è°ƒåº¦ ?

k8s é»˜è®¤çš„è°ƒåº¦å™¨ä¸º `default-scheduler`, è€Œä½¿ç”¨ç›¸åŒè°ƒåº¦å™¨åªèƒ½å•å¹¶å‘å¤„ç†è°ƒåº¦. ä½†æ˜¯å¯ä»¥ä½¿ç”¨è‡ªå®šä¹‰å®ç°è°ƒåº¦å™¨çš„æ–¹æ¡ˆ, åœ¨åˆ›å»º pod æ—¶æŒ‡å®šä¸åŒçš„è°ƒåº¦å™¨ç®—æ³• `pod.Spec.schedulerName = xiaorui.cc`, è¿™æ ·å¯ä»¥ä½¿ä¸åŒè°ƒåº¦å™¨çš„ kube-schedueler å¹¶è¡Œè°ƒåº¦èµ·æ¥, å„è‡ªæŒ‰ç…§è°ƒåº¦ç®—æ³•æ¥è°ƒåº¦, è¿è¡Œäº’ä¸å½±å“.

å½“ç„¶å¤§å¤šæ•°å…¬å¸æ²¡è¿™ä¸ªå¿…è¦.

## schedulingCycle æ ¸å¿ƒè°ƒåº¦å‘¨æœŸçš„å®ç°

`schedulingCycle()` è¯¥æ–¹æ³•ä¸»è¦ä¸º pod é€‰å‡ºæœ€ä¼˜çš„ node èŠ‚ç‚¹. å…ˆé€šè¿‡é¢„é€‰è¿‡ç¨‹è¿‡æ»¤å‡ºç¬¦åˆ pod è¦æ±‚çš„èŠ‚ç‚¹é›†åˆ, å†é€šè¿‡æ’ä»¶å¯¹è¿™äº›èŠ‚ç‚¹è¿›è¡Œæ‰“åˆ†, ä½¿ç”¨åˆ†å€¼æœ€é«˜çš„ node ä¸º pod è°ƒåº¦èŠ‚ç‚¹.

![](https://xiaorui-cc.oss-cn-hangzhou.aliyuncs.com/images/202301/202301251629071.png)

scheduler å†…ç½®å„ä¸ªé˜¶æ®µçš„å„ç§æ’ä»¶, é¢„é€‰å’Œä¼˜é€‰é˜¶æ®µå°±æ˜¯éå†å›è°ƒæ’ä»¶æ±‚å‡ºç»“æœ.

è°ƒåº¦å‘¨æœŸ `schedulingCycle` å†…å…³é”®æ–¹æ³•æ˜¯ `schedulePod`, å…¶ç®€åŒ–æµç¨‹å¦‚ä¸‹.

1. å…ˆè°ƒç”¨ `findNodesThatFitPod` è¿‡æ»¤å‡ºç¬¦åˆè¦æ±‚çš„é¢„é€‰èŠ‚ç‚¹.
2. è°ƒç”¨ `prioritizeNodes` ä¸ºé¢„é€‰å‡ºæ¥çš„èŠ‚ç‚¹è¿›è¡Œæ‰“åˆ† score.
3. æœ€åè°ƒç”¨ `selectHost` é€‰æ‹©æœ€åˆé€‚çš„ node èŠ‚ç‚¹.

```go
func (sched *Scheduler) schedulingCycle(
	...
) (ScheduleResult, *framework.QueuedPodInfo, error) {

	pod := podInfo.Pod

	// é€‰æ‹©èŠ‚ç‚¹
	scheduleResult, err := sched.SchedulePod(ctx, fwk, state, pod)
	...

	// åœ¨ç¼“å­˜ cache ä¸­æ›´æ–°çŠ¶æ€
	err = sched.assume(assumedPod, scheduleResult.SuggestedHost)
	...
}

func (sched *Scheduler) schedulePod(ctx context.Context, fwk framework.Framework, state *framework.CycleState, pod *v1.Pod) (result ScheduleResult, err error) {
	// æ›´æ–°å¿«ç…§
	if err := sched.Cache.UpdateSnapshot(sched.nodeInfoSnapshot); err != nil {
		return result, err
	}

	// è¿›è¡Œé¢„é€‰ç­›é€‰
	feasibleNodes, diagnosis, err := sched.findNodesThatFitPod(ctx, fwk, state, pod)
	if err != nil {
		return result, err
	}

	// é¢„é€‰ä¸‹æ¥ï¼Œæ— èŠ‚ç‚¹å¯ä»¥ç”¨, è¿”å›é”™è¯¯
	if len(feasibleNodes) == 0 {
		return result, &framework.FitError{
			Pod:         pod,
			NumAllNodes: sched.nodeInfoSnapshot.NumNodes(),
			Diagnosis:   diagnosis,
		}
	}

	// ç»è¿‡é¢„é€‰å°±åªæœ‰ä¸€ä¸ªèŠ‚ç‚¹ï¼Œé‚£ä¹ˆç›´æ¥è¿”å›
	if len(feasibleNodes) == 1 {
		return ScheduleResult{
			SuggestedHost:  feasibleNodes[0].Name,
			EvaluatedNodes: 1 + len(diagnosis.NodeToStatusMap),
			FeasibleNodes:  1,
		}, nil
	}

	// è¿›è¡Œä¼˜é€‰è¿‡ç¨‹, å¯¹é¢„é€‰å‡ºæ¥çš„èŠ‚ç‚¹è¿›è¡Œæ‰“åˆ†
	priorityList, err := prioritizeNodes(ctx, sched.Extenders, fwk, state, pod, feasibleNodes)
	if err != nil {
		return result, err
	}

	// ä»ä¼˜é€‰ä¸­é€‰å‡ºæœ€é«˜åˆ†çš„èŠ‚ç‚¹
	host, err := selectHost(priorityList)

	return ScheduleResult{
		SuggestedHost:  host,
		EvaluatedNodes: len(feasibleNodes) + len(diagnosis.NodeToStatusMap),
		FeasibleNodes:  len(feasibleNodes),
	}, err
}
```

### findNodesThatFitPod

`findNodesThatFitPod` æ–¹æ³•ç”¨æ¥å®ç°è°ƒåº¦å™¨çš„é¢„é€‰è¿‡ç¨‹, å…¶å†…éƒ¨è°ƒç”¨æ’ä»¶çš„ PreFilter å’Œ Filter æ–¹æ³•æ¥ç­›é€‰å‡ºç¬¦åˆ pod è¦æ±‚çš„ node èŠ‚ç‚¹é›†åˆ.

```go
func (sched *Scheduler) findNodesThatFitPod(ctx context.Context, fwk framework.Framework, state *framework.CycleState, pod *v1.Pod) ([]*v1.Node, framework.Diagnosis, error) {
	diagnosis := framework.Diagnosis{
		NodeToStatusMap:      make(framework.NodeToStatusMap),
		UnschedulablePlugins: sets.NewString(),
	}

	// è·å–æ‰€æœ‰çš„ nodes
	allNodes, err := sched.nodeInfoSnapshot.NodeInfos().List()
	if err != nil {
		return nil, diagnosis, err
	}

	// è°ƒç”¨ framework çš„ PreFilter é›†åˆé‡Œçš„æ’ä»¶
	preRes, s := fwk.RunPreFilterPlugins(ctx, state, pod)
	if !s.IsSuccess() {
		// å¦‚æœåœ¨ prefilter æœ‰å¼‚å¸¸, åˆ™ç›´æ¥è·³å‡º.
		if !s.IsUnschedulable() {
			return nil, diagnosis, s.AsError()
		}
		msg := s.Message()
		diagnosis.PreFilterMsg = msg
		return nil, diagnosis, nil
	}

	...

	// æ ¹æ® prefilter æ‹¿åˆ°çš„ node names è·å– node info å¯¹è±¡.
	nodes := allNodes
	if !preRes.AllNodes() {
		nodes = make([]*framework.NodeInfo, 0, len(preRes.NodeNames))
		for n := range preRes.NodeNames {
			nInfo, err := sched.nodeInfoSnapshot.NodeInfos().Get(n)
			if err != nil {
				return nil, diagnosis, err
			}
			nodes = append(nodes, nInfo)
		}
	}

	// è¿è¡Œ framework çš„ filter æ’ä»¶åˆ¤æ–­ node æ˜¯å¦å¯ä»¥è¿è¡Œæ–° pod.
	feasibleNodes, err := sched.findNodesThatPassFilters(ctx, fwk, state, pod, diagnosis, nodes)

	...

	// è°ƒç”¨é¢å¤–çš„ extender è°ƒåº¦å™¨æ¥è¿›è¡Œé¢„é€‰
	feasibleNodes, err = findNodesThatPassExtenders(sched.Extenders, pod, feasibleNodes, diagnosis.NodeToStatusMap)
	if err != nil {
		return nil, diagnosis, err
	}
	return feasibleNodes, diagnosis, nil
}
```

#### findNodesThatPassFilters å¹¶å‘æ‰§è¡Œ Filter æ’ä»¶æ–¹æ³•

![](https://xiaorui-cc.oss-cn-hangzhou.aliyuncs.com/images/202301/202301252324752.png)

`findNodesThatPassFilters` æ–¹æ³•ç”¨æ¥éå†æ‰§è¡Œ framework é‡Œ Filter æ’ä»¶é›†åˆçš„ Filter æ–¹æ³•.

ä¸ºäº†åŠ å¿«æ‰§è¡Œæ•ˆç‡, å‡å°‘é¢„é€‰é˜¶æ®µçš„æ—¶å»¶, framework å†…éƒ¨æœ‰ä¸ª Parallelizer å¹¶å‘æ§åˆ¶å™¨, å¯ç”¨ 16 ä¸ªåç¨‹å¹¶å‘è°ƒç”¨æ’ä»¶çš„ Filter æ–¹æ³•. åœ¨å¤§é›†ç¾¤ä¸‹ nodes èŠ‚ç‚¹ä¼šå¾ˆå¤š, ä¸ºäº†é¿å…éå†å…¨é‡çš„ nodes æ‰§è¡Œ Filter å’Œåç»­çš„æ’ä»¶é€»è¾‘, è¿™é‡Œé€šè¿‡ `numFeasibleNodesToFind` æ–¹æ³•æ¥å‡å°‘æ‰«æè®¡ç®—çš„ nodes æ•°é‡.

å½“æˆåŠŸæ‰§è¡Œ filter æ’ä»¶æ–¹æ³•çš„æ•°é‡è¶…è¿‡ numNodesToFind æ—¶, åˆ™æ‰§è¡Œ context cancel(). è¿™æ · framework å¹¶å‘åç¨‹æ± ç›‘å¬åˆ° ctx è¢«å…³é—­å, åˆ™ä¸ä¼šç»§ç»­æ‰§è¡Œåç»­çš„ä»»åŠ¡.

```go
func (sched *Scheduler) findNodesThatPassFilters(
	ctx context.Context,
	fwk framework.Framework,
	pod *v1.Pod,
	nodes []*framework.NodeInfo) ([]*v1.Node, error) {

	numAllNodes := len(nodes)
	// è®¡ç®—éœ€è¦æ‰«æçš„ nodes æ•°, é¿å…äº†è¶…å¤§é›†ç¾¤ä¸‹ nodes çš„è®¡ç®—æ•°é‡.
	// å½“é›†ç¾¤çš„èŠ‚ç‚¹æ•°å°äº 100 æ—¶, åˆ™ç›´æ¥ä½¿ç”¨é›†ç¾¤çš„èŠ‚ç‚¹æ•°ä½œä¸ºæ‰«ææ•°æ®é‡
	// å½“å¤§äº 100 æ—¶, åˆ™ä½¿ç”¨å…¬å¼è®¡ç®— `numAllNodes * (50 - numAllNodes/125) / 100`
	numNodesToFind := sched.numFeasibleNodesToFind(fwk.PercentageOfNodesToScore(), int32(numAllNodes))

	feasibleNodes := make([]*v1.Node, numNodesToFind)

	// å¦‚æœ framework æœªæ³¨å†Œ Filter æ’ä»¶, åˆ™é€€å‡º.
	if !fwk.HasFilterPlugins() {
		for i := range feasibleNodes {
			feasibleNodes[i] = nodes[(sched.nextStartNodeIndex+i)%numAllNodes].Node()
		}
		return feasibleNodes, nil
	}

	// framework å†…ç½®å¹¶å‘æ§åˆ¶å™¨, å¹¶å‘ 16 ä¸ªåç¨‹å»è¯·æ±‚æ’ä»¶çš„ Filter æ–¹æ³•.
	errCh := parallelize.NewErrorChannel()
	var statusesLock sync.Mutex

	checkNode := func(i int) {
		// è·å– node info å¯¹è±¡
		nodeInfo := nodes[(sched.nextStartNodeIndex+i)%numAllNodes]

		// éå†æ‰§è¡Œ framework çš„ Filter æ’ä»¶çš„ Filter æ–¹æ³•.
		status := fwk.RunFilterPluginsWithNominatedPods(ctx, state, pod, nodeInfo)
		if status.Code() == framework.Error {
			// å¦‚æœæœ‰é”™è¯¯, ç›´æ¥æŠŠé”™è¯¯ä¼ åˆ° errCh ç®¡é“é‡Œ.
			errCh.SendErrorWithCancel(status.AsError(), cancel)
			return
		}
		if status.IsSuccess() {
			// å¦‚æœæˆåŠŸæ‰§è¡Œ Filter æ’ä»¶çš„æ•°é‡è¶…è¿‡ numNodesToFind, åˆ™æ‰§è¡Œ cancel().
			// å½“ ctx è¢« cancel(), framework çš„å¹¶å‘åç¨‹æ± ä¸ä¼šç»§ç»­æ‰§è¡Œåç»­çš„ä»»åŠ¡.
			length := atomic.AddInt32(&feasibleNodesLen, 1)
			if length > numNodesToFind {
				cancel()
				atomic.AddInt32(&feasibleNodesLen, -1)
			} else {
				feasibleNodes[length-1] = nodeInfo.Node()
			}
		}
		...
	}

	// å¹¶å‘è°ƒç”¨ framework çš„ Filter æ’ä»¶çš„ Filter æ–¹æ³•.
	fwk.Parallelizer().Until(ctx, numAllNodes, checkNode, frameworkruntime.Filter)
	feasibleNodes = feasibleNodes[:feasibleNodesLen]
	if err := errCh.ReceiveError(); err != nil {
		// å½“æœ‰é”™è¯¯æ—¶, ç›´æ¥è¿”å›.
		statusCode = framework.Error
		return feasibleNodes, err
	}

	// è¿”å›å¯ç”¨çš„ nodes åˆ—è¡¨.
	return feasibleNodes, nil
}
```

framework çš„å¹¶å‘åº“ä¹Ÿæ˜¯é€šè¿‡å°è£… `workqueue.ParallelizeUntil` æ¥å®ç°çš„, å…³äº `parallelizer` çš„å®ç°åŸç†è¿™é‡Œå°±ä¸åšåˆ†æäº†.

`k8s.io/client-go/util/workqueue/parallelizer.go`

#### numFeasibleNodesToFind è®¡ç®—å¤šå°‘èŠ‚ç‚¹å‚ä¸é¢„é€‰

![](https://xiaorui-cc.oss-cn-hangzhou.aliyuncs.com/images/202301/202301261120469.png)

ğŸ¤” è€ƒè™‘ä¸€ä¸ªé—®é¢˜, å½“ k8s çš„ node èŠ‚ç‚¹ç‰¹åˆ«å¤šæ—¶, è¿™äº›èŠ‚ç‚¹éƒ½è¦å‚ä¸é¢„å…ˆçš„è°ƒåº¦è¿‡ç¨‹ä¹ˆ ? æ¯”å¦‚å¤§é›†ç¾¤æœ‰ 2500 ä¸ªèŠ‚ç‚¹, æ³¨å†Œçš„æ’ä»¶æœ‰ 10 ä¸ª, é‚£ä¹ˆ ç­›é€‰ Filter å’Œ æ‰“åˆ† Score è¿‡ç¨‹éœ€è¦è¿›è¡Œ 2500 * 10 * 2 = 50000 æ¬¡è®¡ç®—, æœ€åé€‰å®šä¸€ä¸ªæœ€é«˜åˆ†å€¼çš„èŠ‚ç‚¹æ¥ç»‘å®š pod. k8s scheduler è€ƒè™‘åˆ°äº†è¿™æ ·çš„æ€§èƒ½å¼€é”€, æ‰€ä»¥åŠ å…¥äº†ç™¾åˆ†æ¯”å‚æ•°æ§åˆ¶å‚ä¸é¢„é€‰çš„èŠ‚ç‚¹æ•°.

`numFeasibleNodesToFind` æ–¹æ³•æ ¹æ®å½“å‰é›†ç¾¤çš„èŠ‚ç‚¹æ•°è®¡ç®—å‡ºå‚ä¸é¢„é€‰çš„èŠ‚ç‚¹æ•°é‡, æŠŠå‚ä¸ Filter çš„èŠ‚ç‚¹èŒƒå›´ç¼©å°, æ— éœ€å…¨é¢æ‰«ææ‰€æœ‰çš„èŠ‚ç‚¹, è¿™æ ·é¿å… k8s é›†ç¾¤ nodes å¤ªå¤šæ—¶, é€ æˆæ— æ•ˆçš„è®¡ç®—èµ„æºå¼€é”€.

`numFeasibleNodesToFind` ç­–ç•¥æ˜¯è¿™æ ·çš„, å½“é›†ç¾¤èŠ‚ç‚¹å°äº 100 æ—¶, é›†ç¾¤ä¸­çš„æ‰€æœ‰èŠ‚ç‚¹éƒ½å‚ä¸é¢„é€‰. è€Œå½“å¤§äº 100 æ—¶, åˆ™ä½¿ç”¨ä¸‹é¢çš„å…¬å¼è®¡ç®—æ‰«ææ•°. scheudler çš„ `percentageOfNodesToScore` å‚æ•°é»˜è®¤ä¸º 0, æºç ä¸­ä¼šèµ‹å€¼ä¸º 50 %.

```
numAllNodes * (50 - numAllNodes/125) / 100
```

å‡è®¾å½“å‰é›†ç¾¤æœ‰ `500` ä¸ª nodes èŠ‚ç‚¹, é‚£ä¹ˆéœ€è¦æ‰§è¡Œ Filter æ’ä»¶æ–¹æ³•çš„ nodee èŠ‚ç‚¹æœ‰ `500 * (50 - 500/125) / 100 = 230` ä¸ª.

`numFeasibleNodesToFind` åªæ˜¯è¡¨æ˜æ‰«åˆ°è¿™ä¸ªèŠ‚ç‚¹æ•°åå°±ç»“æŸäº†, ä½†å¦‚æœå‰é¢æ‰§è¡Œæ’ä»¶å‘ç”Ÿå¤±è´¥æ—¶, è‡ªç„¶ä¼šåŠ å¤§æ‰«ææ•°.

```go
func (sched *Scheduler) numFeasibleNodesToFind(percentageOfNodesToScore *int32, numAllNodes int32) (numNodes int32) {
	// å½“å‰çš„é›†ç¾¤å°äº 100 æ—¶, åˆ™ç›´æ¥ä½¿ç”¨é›†ç¾¤èŠ‚ç‚¹æ•°ä½œä¸ºæ‰«ææ•°
	if numAllNodes < minFeasibleNodesToFind {
		return numAllNodes
	}

	// k8s scheduler çš„ nodes ç™¾åˆ†æ¯”é»˜è®¤ä¸º 0
	var percentage int32
	if percentageOfNodesToScore != nil {
		percentage = *percentageOfNodesToScore
	} else {
		percentage = sched.percentageOfNodesToScore
	}

	if percentage == 0 {
		percentage = int32(50) - numAllNodes/125
		// ä¸èƒ½å°äº 5
		if percentage < minFeasibleNodesPercentageToFind {
			percentage = minFeasibleNodesPercentageToFind
		}
	}

	// éœ€è¦æ‰«æçš„èŠ‚ç‚¹æ•°
	numNodes = numAllNodes * percentage / 100
	// å¦‚æœå°äº 100, åˆ™ä½¿ç”¨ 100 ä½œä¸ºæ‰«ææ•°. 
	if numNodes < minFeasibleNodesToFind {
		return minFeasibleNodesToFind
	}

	return numNodes
}
```

### prioritizeNodes

`prioritizeNodes` æ–¹æ³•ä¸ºè°ƒåº¦å™¨çš„ä¼˜é€‰é˜¶æ®µçš„å®ç°. å…¶å†…éƒ¨ä¼šéå†è°ƒç”¨ framework çš„ PreScore æ’ä»¶é›†åˆé‡Œ `PeScore` æ–¹æ³•, ç„¶åå†éå†è°ƒç”¨ framework çš„ Score æ’ä»¶é›†åˆçš„ `Score` æ–¹æ³•. ç»è¿‡ Score æ‰“åˆ†è®¡ç®—åå¯ä»¥æ‹¿åˆ°å„ä¸ª node çš„åˆ†å€¼.

```go
func prioritizeNodes(
	ctx context.Context,
	extenders []framework.Extender,
	fwk framework.Framework,
	state *framework.CycleState,
	pod *v1.Pod,
	nodes []*v1.Node,
) ([]framework.NodePluginScores, error) {
	// å¦‚æœ extenders ä¸ºç©ºå’Œscore æ’ä»¶ä¸ºç©º, åˆ™è·³å‡º
	if len(extenders) == 0 && !fwk.HasScorePlugins() {
		result := make([]framework.NodePluginScores, 0, len(nodes))
		for i := range nodes {
			result = append(result, framework.NodePluginScores{
				Name:       nodes[i].Name,
				TotalScore: 1,
			})
		}
		return result, nil
	}

	// åœ¨ framework çš„ PreScore æ’ä»¶é›†åˆé‡Œ, éå†æ‰§è¡Œæ’ä»¶çš„ PreSocre æ–¹æ³•
	preScoreStatus := fwk.RunPreScorePlugins(ctx, state, pod, nodes)
	if !preScoreStatus.IsSuccess() {
		// åªæœ‰æœ‰å¼‚å¸¸ç›´æ¥é€€å‡º
		return nil, preScoreStatus.AsError()
	}

	// åœ¨ framework çš„ Score æ’ä»¶é›†åˆé‡Œ, éå†æ‰§è¡Œæ’ä»¶çš„ Socre æ–¹æ³•
	nodesScores, scoreStatus := fwk.RunScorePlugins(ctx, state, pod, nodes)
	if !scoreStatus.IsSuccess() {
		return nil, scoreStatus.AsError()
	}

	klogV := klog.V(10)
	if klogV.Enabled() {
		for _, nodeScore := range nodesScores {
			// æ‰“å°æ’ä»¶åå­—å’Œåˆ†å€¼ score
			for _, pluginScore := range nodeScore.Scores {
				klogV.InfoS("Plugin scored node for pod", "pod", klog.KObj(pod), "plugin", pluginScore.Name, "node", nodeScore.Name, "score", pluginScore.Score)
			}
		}
	}

	if len(extenders) != 0 && nodes != nil {
		// å½“é¢å¤– extenders è°ƒåº¦å™¨ä¸ä¸ºç©ºæ—¶, åˆ™éœ€è¦è®¡ç®—åˆ†å€¼.
		...
		...
	}

	return nodesScores, nil
}
```

### selectHost

`selectHost` æ˜¯ä»ä¼˜é€‰çš„ nodes é›†åˆé‡Œè·å–åˆ†å€¼ socre æœ€é«˜çš„ node. å†…éƒ¨è¿˜åšäº†ä¸€ä¸ªå°ä¼˜åŒ–, å½“ç›¸è¿‘çš„ä¸¤ä¸ª node åˆ†å€¼ç›¸åŒæ—¶, åˆ™é€šè¿‡éšæœºæ¥é€‰æ‹© node, ç›®çš„ä½¿ k8s node çš„è´Ÿè½½æ›´è¶‹äºå‡è¡¡.

```go
func selectHost(nodeScores []framework.NodePluginScores) (string, error) {
	// å¦‚æœ nodes ä¸ºç©º, åˆ™è¿”å›é”™è¯¯
	if len(nodeScores) == 0 {
		return "", fmt.Errorf("empty priorityList")
	}

	// ç›´æ¥ä»å¤´åˆ°ä½éå† nodeScores æ•°ç»„, æ‹¿åˆ°åˆ†å€¼ score æœ€åçš„ nodeName.
	maxScore := nodeScores[0].TotalScore
	selected := nodeScores[0].Name
	cntOfMaxScore := 1
	for _, ns := range nodeScores[1:] {
		if ns.TotalScore > maxScore {
			// å½“å‰çš„åˆ†å€¼æ›´å¤§, åˆ™è¿›è¡Œèµ‹å€¼.
			maxScore = ns.TotalScore
			selected = ns.Name
			cntOfMaxScore = 1
		} else if ns.TotalScore == maxScore {
			// å½“ä¸¤ä¸ª node çš„ åˆ†å€¼ç›¸åŒæ—¶, 
			// ä½¿ç”¨éšæœºç®—æ³•æ¥é€‰æ‹©å½“å‰å’Œä¸Šä¸€ä¸ª node.
			cntOfMaxScore++
			if rand.Intn(cntOfMaxScore) == 0 {
				selected = ns.Name
			}
		}
	}

	// è¿”å›åˆ†å€¼æœ€é«˜çš„ node
	return selected, nil
}
```

### PriorityQueue çš„å®ç°

`PriorityQueue` ç”¨æ¥å®ç°ä¼˜å…ˆçº§é˜Ÿåˆ—, informer ä¼šæ¡ä»¶ pod åˆ° priorityQueue é˜Ÿåˆ—ä¸­, `scheduleOne` ä¼šä»è¯¥é˜Ÿåˆ—ä¸­ pop å¯¹è±¡. åœ¨åˆ›å»ºå»¶è¿Ÿé˜Ÿåˆ—æ—¶ä¼ å…¥ä¸€ä¸ª `less` æ¯”è¾ƒæ–¹æ³•, æ—¶é—´æœ€å°çš„ podInfo æ”¾åœ¨ heap çš„æœ€é¡¶ç«¯.

`flushBackoffQCompleted` ä¼šä¸æ–­çš„æ£€æŸ¥ backoff heap å †é¡¶çš„å…ƒç´ æ˜¯å¦æ»¡è¶³æ¡ä»¶, å½“æ»¡è¶³æ¡ä»¶æŠŠ pod å¯¹è±¡æ‰”åˆ° activeQ é˜Ÿé‡Œ, å¹¶æ¿€æ´»æ¡ä»¶å˜é‡. è¿™é‡Œæ²¡æœ‰é‡‡ç”¨ç›‘å¬ç­‰å¾…å †é¡¶åˆ°æœŸæ—¶é—´çš„æ–¹æ³•ï¼Œè€Œæ˜¯æ¯éš”ä¸€ç§’å»æ£€æŸ¥å †é¡¶çš„ podInfo æ˜¯å¦å·²åˆ°æœŸ `isPodBackingoff`.

backoff æ—¶é•¿æ˜¯ä¾èµ– podInfo.Attempts é‡è¯•æ¬¡æ•°çš„ï¼Œé»˜è®¤æƒ…å†µä¸‹é‡è¯• 5æ¬¡ æ˜¯ 5s, æœ€å¤§ä¸èƒ½è¶…è¿‡ 10s. ä¸»è°ƒåº¦æ–¹æ³• `scheduleOne` æ¯æ¬¡ä»é˜Ÿåˆ—è·å– podInfo æ—¶, å®ƒçš„ Attempts å­—æ®µéƒ½ä¼šåŠ ä¸€.

ä»£ç ä½ç½®: `pkg/scheduler/internal/queue/scheduling_queue.go`

```go
const (
	DefaultPodMaxBackoffDuration time.Duration = 10 * time.Second
)

// å®ä¾‹åŒ–ä¼˜å…ˆçº§é˜Ÿåˆ—ï¼Œè¯¥é˜Ÿåˆ—ä¸­å«æœ‰ activeQ, podBackoffQ, unschedulablePodsé›†åˆ.
func NewPriorityQueue() {
	pq.podBackoffQ = heap.NewWithRecorder(
		podInfoKeyFunc, 
		pq.podsCompareBackoffCompleted, 
		...
	)
}

// æ·»åŠ  pod å¯¹è±¡
func (p *PriorityQueue) Add(pod *v1.Pod) error {
	p.lock.Lock()
	defer p.lock.Unlock()

	pInfo := p.newQueuedPodInfo(pod)
	gated := pInfo.Gated

	// æŠŠå¯¹è±¡åŠ åˆ° activeQ é˜Ÿåˆ—é‡Œ.
	if added, err := p.addToActiveQ(pInfo); !added {
		return err
	}
	// å¦‚æœè¯¥å¯¹è±¡åœ¨ä¸å¯è°ƒåº¦é›†åˆä¸­å­˜åœ¨, åˆ™éœ€è¦åœ¨é‡Œé¢åˆ é™¤.
	if p.unschedulablePods.get(pod) != nil {
		p.unschedulablePods.delete(pod, gated)
	}

	// ä» backoffQ åˆ é™¤ pod å¯¹è±¡
	if err := p.podBackoffQ.Delete(pInfo); err == nil {
		klog.ErrorS(nil, "Error: pod is already in the podBackoff queue", "pod", klog.KObj(pod))
	}

	// æ¡ä»¶å˜é‡é€šçŸ¥
	p.cond.Broadcast()

	return nil
}

// è·å– pod info å¯¹è±¡
func (p *PriorityQueue) Pop() (*framework.QueuedPodInfo, error) {
	p.lock.Lock()
	defer p.lock.Unlock()

	// å¦‚æœ activeQ ä¸ºç©º, é™·å…¥ç­‰å¾…
	for p.activeQ.Len() == 0 {
		if p.closed {
			return nil, fmt.Errorf(queueClosed)
		}
		p.cond.Wait()
	}

	// ä» activeQ å †é¡¶ pop å¯¹è±¡
	obj, err := p.activeQ.Pop()
	if err != nil {
		return nil, err
	}

	pInfo := obj.(*framework.QueuedPodInfo)
	// åŠ ä¸€
	pInfo.Attempts++
	p.schedulingCycle++
	return pInfo, nil
}

// heap çš„æ¯”è¾ƒæ–¹æ³•ï¼Œç¡®ä¿ deadline æœ€ä½çš„åœ¨ heap é¡¶éƒ¨.
func (p *PriorityQueue) podsCompareBackoffCompleted(podInfo1, podInfo2 interface{}) bool {
	bo1 := p.getBackoffTime(pInfo1)
	bo2 := p.getBackoffTime(pInfo2)
	return bo1.Before(bo2)
}

func (p *PriorityQueue) getBackoffTime(podInfo *framework.QueuedPodInfo) time.Time {
	duration := p.calculateBackoffDuration(podInfo)
	backoffTime := podInfo.Timestamp.Add(duration)
	return backoffTime
}

// backoff duration éšç€é‡è¯•æ¬¡æ•°ä¸æ–­å åŠ ï¼Œä½†æœ€å¤§ä¸èƒ½è¶…è¿‡ maxBackoffDuration.
func (p *PriorityQueue) calculateBackoffDuration(podInfo *framework.QueuedPodInfo) time.Duration {
	duration := p.podInitialBackoffDuration // 1s
	for i := 1; i < podInfo.Attempts; i++ {
		if duration > p.podMaxBackoffDuration-duration {
			return p.podMaxBackoffDuration // 10s
		}
		duration += duration
	}
	return duration
}
```

### å¦‚ä½•å¤„ç†è°ƒåº¦å¤±è´¥çš„ pod

å‰é¢æœ‰è¯´ kubernetes scheduler çš„ `scheduleOne` ä½œä¸ºä¸»å¾ªç¯å¤„ç†å‡½æ•°ï¼Œå½“æ²¡æœ‰ä¸º pod æ‰¾åˆ°åˆé€‚ node æ—¶ï¼Œä¼šè°ƒç”¨ `FailureHandler` æ–¹æ³•.

`FailureHandler()` æ˜¯ç”± `handleSchedulingFailure()` æ–¹æ³•å®ç°. è¯¥é€»è¾‘çš„å®ç°ç®€å•è¯´å°±æ˜¯æŠŠå¤±è´¥çš„ pod æ‰”åˆ° podBackoffQ é˜Ÿåˆ—æˆ–è€… unschedulablePods é›†åˆé‡Œ.

```go
func (sched *Scheduler) handleSchedulingFailure(ctx context.Context, fwk framework.Framework, podInfo *framework.QueuedPodInfo, err error, reason string, nominatingInfo *framework.NominatingInfo, start time.Time) {
	podLister := fwk.SharedInformerFactory().Core().V1().Pods().Lister()
	cachedPod, e := podLister.Pods(pod.Namespace).Get(pod.Name)
	if e != nil {
	} else {
		if len(cachedPod.Spec.NodeName) != 0 {
			klog.InfoS("Pod has been assigned to node. Abort adding it back to queue.", "pod", klog.KObj(pod), "node", cachedPod.Spec.NodeName)
		} else {
			podInfo.PodInfo, _ = framework.NewPodInfo(cachedPod.DeepCopy())

			// é‡æ–°å…¥é˜Ÿåˆ—
			if err := sched.SchedulingQueue.AddUnschedulableIfNotPresent(podInfo, sched.SchedulingQueue.SchedulingCycle()); err != nil {
				klog.ErrorS(err, "Error occurred")
			}
		}
	}

	...
}

func (p *PriorityQueue) AddUnschedulableIfNotPresent(pInfo *framework.QueuedPodInfo, podSchedulingCycle int64) error {
	pod := pInfo.Pod

	// å»é‡åˆ¤æ–­
	if _, exists, _ := p.activeQ.Get(pInfo); exists {
		return fmt.Errorf("Pod %v is already present in the active queue", klog.KObj(pod))
	}
	// å»é‡åˆ¤æ–­
	if _, exists, _ := p.podBackoffQ.Get(pInfo); exists {
		return fmt.Errorf("Pod %v is already present in the backoff queue", klog.KObj(pod))
	}

	// æŠŠæ²¡æœ‰è°ƒåº¦æˆåŠŸçš„ podInfo æ‰”åˆ° backoffQ é˜Ÿåˆ—æˆ–è€… unschedulablePods é›†åˆä¸­.
	if p.moveRequestCycle >= podSchedulingCycle {
		if err := p.podBackoffQ.Add(pInfo); err != nil {
			return fmt.Errorf("error adding pod %v to the backoff queue: %v", klog.KObj(pod), err)
		}
	} else {
		p.unschedulablePods.addOrUpdate(pInfo)
	}

	return nil
}
```

scheduler queue åœ¨å¯åŠ¨æ—¶ä¼šå¼€å¯ä¸¤ä¸ªå¸¸é©»çš„åç¨‹. ä¸€ä¸ªåç¨‹æ¥ç®¡ç† `flushBackoffQCompleted()`ï¼Œæ¯éš”ä¸€ç§’æ¥è°ƒç”¨ä¸€æ¬¡. å¦ä¸€ä¸ªåç¨‹æ¥ç®¡ç† `flushUnschedulablePodsLeftover`, æ¯éš”ä¸‰åç§’æ¥è°ƒç”¨ä¸€æ¬¡.

```go
// Run starts the goroutine to pump from podBackoffQ to activeQ
func (p *PriorityQueue) Run() {
	go wait.Until(p.flushBackoffQCompleted, 1.0*time.Second, p.stop)
	go wait.Until(p.flushUnschedulablePodsLeftover, 30*time.Second, p.stop)
}
```

`flushBackoffQCompleted` ä» podBackoffQ è·å– podInfo, ç„¶åæ‰”åˆ° activeQ é‡Œï¼Œç­‰å¾… scheduleOne æ¥è°ƒåº¦å¤„ç†.

```go
func (p *PriorityQueue) flushBackoffQCompleted() {
	activated := false
	for {
		rawPodInfo := p.podBackoffQ.Peek()
		pInfo := rawPodInfo.(*framework.QueuedPodInfo)
		pod := pInfo.Pod
        
		// ä» podBackoffQ ä¸­è·å–ä¸Šæ¬¡è°ƒåº¦å¤±è´¥çš„ podInfo.
		_, err := p.podBackoffQ.Pop()
		if err != nil {
			klog.ErrorS(err, "Unable to pop pod from backoff queue despite backoff completion", "pod", klog.KObj(pod))
			break
		}
        
		// ç„¶åæŠŠè¿™ podInfo å†æ‰”åˆ° activeQ é‡Œï¼Œè®© scheduleOne ä¸»å¾ªç¯æ¥å¤„ç†.
		if added, _ := p.addToActiveQ(pInfo); added {
			klog.V(5).InfoS("Pod moved to an internal scheduling queue", "pod", klog.KObj(pod), "event", BackoffComplete, "queue", activeQName)
			activated = true
		}
	}

	// å¦‚æœæœ‰æ·»åŠ æˆåŠŸçš„ï¼Œåˆ™æ¿€æ´»æ¡ä»¶å˜é‡.
	if activated {
		p.cond.Broadcast()
	}
}
```

`flushUnschedulablePodsLeftover` åŠ é”éå† PodInfoMap, å¦‚æœæŸä¸ª pod çš„è·ç¦»ä¸Šæ¬¡çš„è°ƒåº¦æ—¶é—´å¤§äº60s, åˆ™æ‰”åˆ°ä¸¤ä¸ªé˜Ÿåˆ—ä¸­çš„ä¸€ä¸ªï¼Œå¦åˆ™ç­‰å¾…ä¸‹ä¸ª 30s å†æ¥å¤„ç†.

```go
func (p *PriorityQueue) flushUnschedulablePodsLeftover() {
	p.lock.Lock()
	defer p.lock.Unlock()

	var podsToMove []*framework.QueuedPodInfo
	for _, pInfo := range p.unschedulablePods.podInfoMap {
		lastScheduleTime := pInfo.Timestamp
		if currentTime.Sub(lastScheduleTime) > p.podMaxInUnschedulablePodsDuration {
			podsToMove = append(podsToMove, pInfo)
		}
	}

	if len(podsToMove) > 0 {
		// æŠŠ podInfo æ‰”åˆ° activeQ å’Œ backoffQ é˜Ÿåˆ—ä¸­.
		p.movePodsToActiveOrBackoffQueue(podsToMove, UnschedulableTimeout)
	}
}

func (p *PriorityQueue) movePodsToActiveOrBackoffQueue(podInfoList []*framework.QueuedPodInfo, event framework.ClusterEvent) {
	activated := false
	for _, pInfo := range podInfoList {
		pod := pInfo.Pod
		if p.isPodBackingoff(pInfo) {
			// å¦‚æœè¿˜éœ€è¦ backoff é€€é¿, åˆ™æ‰”åˆ° podBackoffQ é˜Ÿåˆ—ä¸­è¿›è¡Œå»¶è¿Ÿ, åé¢ç”± flushBackoffQCompleted å¤„ç†.
			if err := p.podBackoffQ.Add(pInfo); err != nil {
			} else {
				p.unschedulablePods.delete(pod, pInfo.Gated)
			}
		} else {
			// æŠŠ podInfo æ‰”åˆ° activeQ é‡Œ, ç­‰å¾… scheduleOne è°ƒåº¦å¤„ç†.
			if added, _ := p.addToActiveQ(pInfo); added {
				activated = true
				p.unschedulablePods.delete(pod, gated)
			}
		}
	}
    ...
}
```

## bindingCycle å®ç° pod å’Œ node ç»‘å®š

`bindingCycle` ç”¨æ¥å®ç° pod å’Œ node ç»‘å®š, å…¶è¿‡ç¨‹ä¸º `prebind -> bind -> postbind`.

```go
func (sched *Scheduler) bindingCycle(
	... {

	assumedPod := assumedPodInfo.Pod

	// æ‰§è¡Œæ’ä»¶çš„ prebind é€»è¾‘
	if status := fwk.RunPreBindPlugins(ctx, state, assumedPod, scheduleResult.SuggestedHost); !status.IsSuccess() {
		return status
	}

	// æ‰§è¡Œ bind æ’ä»¶é€»è¾‘
	if status := sched.bind(ctx, fwk, assumedPod, scheduleResult.SuggestedHost, state); !status.IsSuccess() {
		return status
	}

	// åœ¨ bind ç»‘å®šåæ‰§è¡Œæ”¶å°¾æ“ä½œ
	fwk.RunPostBindPlugins(ctx, state, assumedPod, scheduleResult.SuggestedHost)

	return nil
}

func (sched *Scheduler) bind(ctx context.Context, fwk framework.Framework, assumed *v1.Pod, targetNode string, state *framework.CycleState) (status *framework.Status) {
	defer func() {
		sched.finishBinding(fwk, assumed, targetNode, status)
	}()

	bound, err := sched.extendersBinding(assumed, targetNode)
	if bound {
		return framework.AsStatus(err)
	}
	return fwk.RunBindPlugins(ctx, state, assumed, targetNode)
}

func (sched *Scheduler) extendersBinding(pod *v1.Pod, node string) (bool, error) {
	for _, extender := range sched.Extenders {
		if !extender.IsBinder() || !extender.IsInterested(pod) {
			continue
		}
		return true, extender.Bind(&v1.Binding{
			ObjectMeta: metav1.ObjectMeta{Namespace: pod.Namespace, Name: pod.Name, UID: pod.UID},
			Target:     v1.ObjectReference{Kind: "Node", Name: node},
		})
	}
	return false, nil
}
```

extender é»˜è®¤å°±åªæœ‰ `DefaultBinder` æ’ä»¶, è¯¥æ’ä»¶çš„ bind é€»è¾‘æ˜¯é€šè¿‡ clientset å¯¹ pod è¿›è¡Œ node ç»‘å®š..

ä»£ç ä½ç½®: `pkg/scheduler/framework/plugins/defaultbinder/default_binder.go`

```go
func (b DefaultBinder) Bind(ctx context.Context, state *framework.CycleState, p *v1.Pod, nodeName string) *framework.Status {
	binding := &v1.Binding{
		ObjectMeta: metav1.ObjectMeta{Namespace: p.Namespace, Name: p.Name, UID: p.UID},
		Target:     v1.ObjectReference{Kind: "Node", Name: nodeName},
	}
	err := b.handle.ClientSet().CoreV1().Pods(binding.Namespace).Bind(ctx, binding, metav1.CreateOptions{})
	if err != nil {
		return framework.AsStatus(err)
	}
	return nil
}
```

## framework çš„å®ç°åŸç†

k8s scheduler è®¾è®¡äº†ä¸€å¥— framework ä½œä¸ºè°ƒåº¦å™¨çš„æ’ä»¶ç³»ç»Ÿ. å¦å¤–åœ¨ k8s ä¸­å†…ç½®äº†ä¸€æ³¢æ’ä»¶, æ’ä»¶æ˜¯å¯ä»¥åœ¨å¤šä¸ªæ‰©å±•ç‚¹å¤„è¿›è¡Œæ³¨å†Œ. å½“è°ƒåº¦å™¨åœ¨å¯åŠ¨æ—¶ä¼šåŠ è½½æ³¨å†Œæ’ä»¶åˆ° registry, å½“ pod éœ€è¦åˆ›å»ºæ—¶, scheduler é€šè¿‡æ’ä»¶ç³»ç»Ÿè¿‡æ»¤å‡ºé€‚åˆçš„èŠ‚ç‚¹é›†åˆ, ç„¶ååœ¨é€šè¿‡å®ç° socre æ‰“åˆ†çš„æ’ä»¶é€‰å‡ºåˆ†å€¼æœ€é«˜çš„èŠ‚ç‚¹.

![](https://xiaorui-cc.oss-cn-hangzhou.aliyuncs.com/images/202301/202301242208191.png)

### framework å…³é”®æŒ‚è½½ç‚¹åˆ†æ

**PreFilter**

åœ¨è°ƒç”¨ `Filter` æ–¹æ³•å‰, æ‰§è¡Œ `PreFilter` é¢„ç­›é€‰é€»è¾‘. æ’ä»¶ä¼šæ£€æµ‹é›†ç¾¤å’Œ pod æ˜¯å¦æ»¡è¶³å®šä¹‰çš„æ¡ä»¶. å½“ä¸æ»¡è¶³æ—¶è¿”å›é”™è¯¯, åˆ™åœæ­¢è°ƒåº¦å‘¨æœŸ.

**Filter**

è¿™äº›æ’ä»¶ç”¨äºè¿‡æ»¤å‡ºä¸èƒ½è¿è¡Œè¯¥ Pod çš„èŠ‚ç‚¹.

**PostFilter**

è¿™äº›æ’ä»¶åœ¨ Filter é˜¶æ®µåè°ƒç”¨, ä½†ä»…åœ¨è¯¥ Pod æ²¡æœ‰å¯è¡Œçš„èŠ‚ç‚¹æ—¶è°ƒç”¨.

**PreScore**

æ’ä»¶åœ¨æ‰§è¡Œ `Score` æ‰“åˆ†å‰, å¯ä»¥å…ˆè¿›è¡Œ `PreScore` å‰ç½®è¯„åˆ†å·¥ä½œ.

**Score**

å¯¹é€šè¿‡é¢„é€‰é˜¶æ®µçš„èŠ‚ç‚¹é›†åˆè¿›è¡Œæ‰“åˆ†.

## scheduler å†…ç½®æ’ä»¶çš„å®ç°åŸç†

k8s scheduler å†…ç½®äº†ä¸å°‘æ’ä»¶, ä¸‹é¢æ˜¯å†…ç½®çš„æ’ä»¶.

```go
func getDefaultPlugins() *v1beta3.Plugins {
	plugins := &v1beta3.Plugins{
		MultiPoint: v1beta3.PluginSet{
			Enabled: []v1beta3.Plugin{
				{Name: names.PrioritySort},
				{Name: names.NodeUnschedulable},
				{Name: names.NodeName},
				{Name: names.TaintToleration, Weight: pointer.Int32(3)},
				{Name: names.NodeAffinity, Weight: pointer.Int32(2)},
				{Name: names.NodePorts},
				{Name: names.NodeResourcesFit, Weight: pointer.Int32(1)},
				{Name: names.VolumeRestrictions},
				{Name: names.EBSLimits},
				{Name: names.GCEPDLimits},
				{Name: names.NodeVolumeLimits},
				{Name: names.AzureDiskLimits},
				{Name: names.VolumeBinding},
				{Name: names.VolumeZone},
				{Name: names.PodTopologySpread, Weight: pointer.Int32(2)},
				{Name: names.InterPodAffinity, Weight: pointer.Int32(2)},
				{Name: names.DefaultPreemption},
				{Name: names.NodeResourcesBalancedAllocation, Weight: pointer.Int32(1)},
				{Name: names.ImageLocality, Weight: pointer.Int32(1)},
				{Name: names.DefaultBinder},
			},
		},
	}
	...
	return plugins
}
```

è¿™é‡Œé€‰æ‹© `NodeName`, `ImageLocality`, `NodeResourcesFit` å’Œ `NodeAffinity` æ’ä»¶æ¥åˆ†æçš„æ’ä»¶å®ç°åŸç†.

### NodeName æ’ä»¶çš„å®ç°åŸç†

`NodeName` æ’ä»¶æ˜¯ä¸€ä¸ªé¢„é€‰æ’ä»¶, æ’ä»¶å®ç°äº† `Filter` æ–¹æ³•, ç­›é€‰å‡ºè·Ÿ pod ç›¸åŒ nodeName çš„ node èŠ‚ç‚¹å¯¹è±¡.

æºç ä½ç½®: `pkg/scheduler/framework/plugins/nodename/node_name.go`

```go
type NodeName struct{}

var _ framework.FilterPlugin = &NodeName{}

const (
	Name = names.NodeName

	ErrReason = "node(s) didn't match the requested node name"
)

func (pl *NodeName) Name() string {
	return Name
}

func (pl *NodeName) Filter(ctx context.Context, _ *framework.CycleState, pod *v1.Pod, nodeInfo *framework.NodeInfo) *framework.Status {
	if nodeInfo.Node() == nil {
		return framework.NewStatus(framework.Error, "node not found")
	}
	if !Fits(pod, nodeInfo) {
		return framework.NewStatus(framework.UnschedulableAndUnresolvable, ErrReason)
	}
	return nil
}

func Fits(pod *v1.Pod, nodeInfo *framework.NodeInfo) bool {
	// è¿‡æ»¤èŠ‚ç‚¹, å½“è¿”å› true, è¯¥èŠ‚ç‚¹é€šè¿‡äº†åˆé€‰.
	// å½“ pod æ²¡æœ‰æŒ‡å®š nodeName æˆ–è€… pod æŒ‡å®šçš„ nodeName è·Ÿä¼ å…¥çš„ node åå­—ä¸€è‡´åˆ™è¿”å› true.
	return len(pod.Spec.NodeName) == 0 || pod.Spec.NodeName == nodeInfo.Node().Name
}

...
```

### ImageLocality æ’ä»¶çš„å®ç°åŸç†

`ImageLocality` æ’ä»¶å®ç°äº† `Score` åˆ†æ”¯è®¡ç®—æ¥å£, è¯¥æ’ä»¶ä¼šè®¡ç®— pod å…³è”çš„å®¹å™¨çš„é•œåƒåœ¨ node ä¸Šçš„çŠ¶æ€, ç»è¿‡å„ç§æ ¡éªŒå’Œå…¬å¼è®¡ç®—åå¾—å‡ºä¸€ä¸ª score åˆ†å€¼.

è‡³äº `ImageLocality` æ’ä»¶å¦‚ä½•è®¡ç®— score åˆ†å€¼ä»£ç é‡Œå†™æ¸…æ¥šäº†, ä½†è‡³äºä¸ºä»€ä¹ˆè¦è¿™ä¹ˆè®¡ç®—å°±çœ‹ä¸æ˜ç™½äº†.

æºç ä½ç½®: `pkg/scheduler/framework/plugins/imagelocality/image_locality.go`

```go
type ImageLocality struct {
	handle framework.Handle
}

var _ framework.ScorePlugin = &ImageLocality{}

const Name = names.ImageLocality

...

func (pl *ImageLocality) Score(ctx context.Context, state *framework.CycleState, pod *v1.Pod, nodeName string) (int64, *framework.Status) {
	// é€šè¿‡ nodeName è·å– nodeInfo å¯¹è±¡
	nodeInfo, err := pl.handle.SnapshotSharedLister().NodeInfos().Get(nodeName)
	if err != nil {
		return 0, framework.AsStatus(fmt.Errorf("getting node %q from Snapshot: %w", nodeName, err))
	}

	// è·å–ä¸»æœºåˆ—è¡¨
	nodeInfos, err := pl.handle.SnapshotSharedLister().NodeInfos().List()
	if err != nil {
		return 0, framework.AsStatus(err)
	}

	// å½“å‰æœ‰å¤šå°‘ node
	totalNumNodes := len(nodeInfos)

	// ç»è¿‡ä¸€å †è¡¨è¾¾å¼è®¡ç®—å‡º node å¯¹åº”çš„ score åˆ†æ”¯
	score := calculatePriority(sumImageScores(nodeInfo, pod.Spec.Containers, totalNumNodes), len(pod.Spec.Containers))

	return score, nil
}

// æ ¹æ® node å½“å‰é•œåƒçš„çŠ¶æ€è®¡ç®—åˆ†å€¼.
func calculatePriority(sumScores int64, numContainers int) int64 {
	maxThreshold := maxContainerThreshold * int64(numContainers)
	if sumScores < minThreshold {
		sumScores = minThreshold
	} else if sumScores > maxThreshold {
		sumScores = maxThreshold
	}

	return int64(framework.MaxNodeScore) * (sumScores - minThreshold) / (maxThreshold - minThreshold)
}

// éå† pod é‡Œçš„æ‰€æœ‰å®¹å™¨, å½“å®¹å™¨å¯¹åº”çš„é•œåƒåœ¨ node ä¸­, åˆ™ç´¯åŠ è®¡ç®—åˆ†å€¼ score.
func sumImageScores(nodeInfo *framework.NodeInfo, containers []v1.Container, totalNumNodes int) int64 {
	var sum int64
	for _, container := range containers {
		// è·å–å®¹å™¨çš„æ™¯è±¡åœ¨ node çš„çŠ¶æ€
		if state, ok := nodeInfo.ImageStates[normalizedImageName(container.Image)]; ok {
			sum += scaledImageScore(state, totalNumNodes)
		}
	}
	return sum
}
```

### NodeResourcesFit æ’ä»¶çš„å®ç°åŸç†

`NodeResourcesFit` æ’ä»¶å®ç°äº†ä¸‰ä¸ª framework æ’ä»¶æ¥å£, åˆ†åˆ«æ˜¯ PreFilterPlugin, FilterPlugin, ScorePlugin æ¥å£.

- `PreFilterPlugin` åªæ˜¯åœ¨ state ç¼“å­˜é‡Œè®°å½• pod è¦æ±‚çš„ request èµ„æº.
- `FilterPlugin` ä¸»è¦ç”¨æ¥è¿‡æ»¤æ‰ä¸ç¬¦åˆèµ„æºè¦æ±‚çš„ node èŠ‚ç‚¹, å¦‚æœå½“å‰ node å¯ç”³è¯·çš„ cpu å’Œ mem ä¸æ»¡è¶³ pod éœ€æ±‚, åˆ™ node èŠ‚ç‚¹ä¼šè¢«è¿‡æ»¤æ‰.
- `ScorePlugin` ä¸»è¦å¯¹ç¬¦åˆè¦æ±‚é€šè¿‡é¢„é€‰çš„ node é›†åˆè¿›è¡Œæ‰“åˆ†, ç”¨ä»¥å¾—åˆ°æœ€ä¼˜çš„ node èŠ‚ç‚¹, å…¶å®å°±æ˜¯ cpu/mem èµ„æºæœ€å……è¶³æœ€ç©ºé—²çš„ node èŠ‚ç‚¹.

ä»£ç ä½ç½®: `pkg/scheduler/framework/plugins/noderesources/fit.go`

```go
var _ framework.PreFilterPlugin = &Fit{}
var _ framework.FilterPlugin = &Fit{}
var _ framework.ScorePlugin = &Fit{}

...

// ç”¨æ¥åœ¨ cycleState é‡Œè®°å½• pod æ‰€éœ€çš„ request èµ„æº
func (f *Fit) PreFilter(ctx context.Context, cycleState *framework.CycleState, pod *v1.Pod) (*framework.PreFilterResult, *framework.Status) {
	cycleState.Write(preFilterStateKey, computePodResourceRequest(pod))
	return nil, nil
}

// è¿‡æ»¤æ‰ä¸ç¬¦åˆ resource request èµ„æºçš„ node.
func (f *Fit) Filter(ctx context.Context, cycleState *framework.CycleState, pod *v1.Pod, nodeInfo *framework.NodeInfo) *framework.Status {
	// è·å–åœ¨ preFilter é˜¶æ®µå†™å…¥çš„ preFilterState
	s, err := getPreFilterState(cycleState)
	if err != nil {
		return framework.AsStatus(err)
	}

	// åˆ¤æ–­å½“å‰çš„ node æ˜¯å¦æ»¡è¶³ pod çš„èµ„æºè¯·æ±‚éœ€æ±‚
	insufficientResources := fitsRequest(s, nodeInfo, f.ignoredResources, f.ignoredResourceGroups)

	// å¦‚æœä¸ä¸ºç©ºåˆ™æœ‰å¼‚å¸¸, æŠŠæ‰€æœ‰çš„å¼‚å¸¸åˆå¹¶åœ¨ä¸€èµ·, æ„å»º framework status å¯¹è±¡å†è¿”å›.
	if len(insufficientResources) != 0 {
		// We will keep all failure reasons.
		failureReasons := make([]string, 0, len(insufficientResources))
		for i := range insufficientResources {
			failureReasons = append(failureReasons, insufficientResources[i].Reason)
		}
		return framework.NewStatus(framework.Unschedulable, failureReasons...)
	}

	// è¿”å› nil, è¯´æ˜è¯¥èŠ‚ç‚¹ç¬¦åˆèµ„æºè¦æ±‚
	return nil
}

// åˆ¤æ–­å½“å‰ node æ˜¯å¦æ»¡è¶³ pod çš„èµ„æºè¦æ±‚
func fitsRequest(podRequest *preFilterState, nodeInfo *framework.NodeInfo, ignoredExtendedResources, ignoredResourceGroups sets.String) []InsufficientResource {
	insufficientResources := make([]InsufficientResource, 0, 4)
	allowedPodNumber := nodeInfo.Allocatable.AllowedPodNumber

	// å¦‚æœå½“å‰ node pods æ•°è¶…è¿‡äº†æœ€å¤§ pods æ•°, åˆ™ append.
	if len(nodeInfo.Pods)+1 > allowedPodNumber {
		insufficientResources = append(insufficientResources, InsufficientResource{
			ResourceName: v1.ResourcePods,
			Reason:       "Too many pods",
			Requested:    1,
			Used:         int64(len(nodeInfo.Pods)),
			Capacity:     int64(allowedPodNumber),
		})
	}

	// å¦‚æœæ²¡æœ‰ pod æ²¡æœ‰ resource request é…ç½®, å¯ç›´æ¥è¿”å›
	if podRequest.MilliCPU == 0 &&
		podRequest.Memory == 0 &&
		podRequest.EphemeralStorage == 0 &&
		len(podRequest.ScalarResources) == 0 {
		return insufficientResources
	}

	// å¦‚æœå½“å‰ node ç©ºé—² cpu èµ„æºä¸è¶³ä»¥è¿è¡Œ pod, åˆ™ append å¼‚å¸¸.
	// å½“å‰çš„ cpu èµ„æºæ˜¯æŒ‰ç…§ request æ¥è®¡ç®—çš„, è€Œä¸æ˜¯ metrics å®æ—¶çš„.
	if podRequest.MilliCPU > (nodeInfo.Allocatable.MilliCPU - nodeInfo.Requested.MilliCPU) {
		insufficientResources = append(insufficientResources, InsufficientResource{
			ResourceName: v1.ResourceCPU,
			Reason:       "Insufficient cpu",
			Requested:    podRequest.MilliCPU,
			Used:         nodeInfo.Requested.MilliCPU,
			Capacity:     nodeInfo.Allocatable.MilliCPU,
		})
	}
	// å¦‚æœå½“å‰ node ç©ºé—²çš„å†…å­˜ä¸æ»¡è¶³ pod çš„è¦æ±‚, åˆ™ append å¼‚å¸¸
	if podRequest.Memory > (nodeInfo.Allocatable.Memory - nodeInfo.Requested.Memory) {
		insufficientResources = append(insufficientResources, InsufficientResource{
			ResourceName: v1.ResourceMemory,
			Reason:       "Insufficient memory",
			Requested:    podRequest.Memory,
			Used:         nodeInfo.Requested.Memory,
			Capacity:     nodeInfo.Allocatable.Memory,
		})
	}
	// å¦‚æœå½“å‰ node çš„å­˜å‚¨ç©ºé—´ä¸å¤Ÿ pod çš„è¦æ±‚, åˆ™è¿”å›é”™è¯¯.
	if podRequest.EphemeralStorage > (nodeInfo.Allocatable.EphemeralStorage - nodeInfo.Requested.EphemeralStorage) {
		insufficientResources = append(insufficientResources, InsufficientResource{
			ResourceName: v1.ResourceEphemeralStorage,
			Reason:       "Insufficient ephemeral-storage",
			Requested:    podRequest.EphemeralStorage,
			Used:         nodeInfo.Requested.EphemeralStorage,
			Capacity:     nodeInfo.Allocatable.EphemeralStorage,
		})
	}

	// è¿™ä¸ªæ˜¯ pod å¯¹æ‰©å±•èµ„æºçš„è¦æ±‚, å½“èŠ‚ç‚¹ä¸æ»¡è¶³å…¶è¦æ±‚æ—¶, append è¿½åŠ å¼‚å¸¸.
	for rName, rQuant := range podRequest.ScalarResources {
		...

		if v1helper.IsExtendedResourceName(rName) {
			var rNamePrefix string
			if ignoredResourceGroups.Len() > 0 {
				rNamePrefix = strings.Split(string(rName), "/")[0]
			}
			if ignoredExtendedResources.Has(string(rName)) || ignoredResourceGroups.Has(rNamePrefix) {
				continue
			}
		}

		if rQuant > (nodeInfo.Allocatable.ScalarResources[rName] - nodeInfo.Requested.ScalarResources[rName]) {
			insufficientResources = append(insufficientResources, InsufficientResource{
				ResourceName: rName,
				Reason:       fmt.Sprintf("Insufficient %v", rName),
				Requested:    podRequest.ScalarResources[rName],
				Used:         nodeInfo.Requested.ScalarResources[rName],
				Capacity:     nodeInfo.Allocatable.ScalarResources[rName],
			})
		}
	}

	// è¿”å›æ‰€æœ‰çš„èµ„æºåˆ†é…çš„å¼‚å¸¸
	return insufficientResources
}
```

`Score` ä¼šæ ¹æ®å½“å‰ node çš„ cpu/mem/ephemeral-storage èµ„æºç©ºé—²æƒ…å†µè¿›è¡Œæ‰“åˆ†, å¯åˆ†é…çš„èµ„æºè¶Šå¤š, é‚£ä¹ˆåˆ†å€¼è‡ªç„¶å°±è¶Šé«˜.

```go
func (f *Fit) Score(ctx context.Context, state *framework.CycleState, pod *v1.Pod, nodeName string) (int64, *framework.Status) {
	// è·å– nodeInfo å¯¹è±¡
	nodeInfo, err := f.handle.SnapshotSharedLister().NodeInfos().Get(nodeName)
	if err != nil {
		return 0, framework.AsStatus(fmt.Errorf("getting node %q from Snapshot: %w", nodeName, err))
	}

	// è®¡ç®—åˆ†å€¼ score
	return f.score(pod, nodeInfo)
}

func (r *resourceAllocationScorer) score(
	pod *v1.Pod,
	nodeInfo *framework.NodeInfo) (int64, *framework.Status) {

	...
	requested := make([]int64, len(r.resources))
	allocatable := make([]int64, len(r.resources))

	// éå† resouces ç´¯åŠ è®¡ç®— allocatable å’Œ requested.
	for i := range r.resources {
		// allocatable æ˜¯ node è¿˜å¯ä»¥åˆ†é…çš„èµ„æº
		// req æ˜¯ pod æ‰€éœ€è¦çš„èµ„æº
		alloc, req := r.calculateResourceAllocatableRequest(nodeInfo, pod, v1.ResourceName(r.resources[i].Name))

		// å¦‚æœä¸º 0, é€šå¸¸ä¸ºæ‰©å±•èµ„æº, è·³è¿‡åˆ†å€¼è®¡ç®—.
		if alloc == 0 {
			continue
		}
		allocatable[i] = alloc
		requested[i] = req
	}

	// é€šè¿‡ä¸€æ³¢å…¬å¼æ¥è®¡ç®—è¯¥ node åˆ†å€¼
	score := r.scorer(requested, allocatable)

	...
	return score, nil
}
```

`NodeResourcesFit` çš„ scorer æ˜¯å…·ä½“ç”¨æ¥æ‰“åˆ†çš„æ–¹æ³•, å…¶å†…éƒ¨æ ¹æ® requested å’Œ allocatable ä¸¤ä¸ªå‚æ•°è¿›è¡Œæ‰“åˆ†.

scheduler å¯¹ resource æ‰“åˆ†å†…ç½®ä¸‰ç§ä¸åŒç­–ç•¥, åˆ†åˆ«æ˜¯ LeastAllocated / MostAllocated / RequestedToCapacityRatio.

- LeastAllocated é»˜è®¤ç­–ç•¥, ç©ºé—²èµ„æºå¤šçš„åˆ†é«˜, ä¼˜å…ˆè°ƒåº¦åˆ°ç©ºé—²èµ„æºå¤šçš„èŠ‚ç‚¹ä¸Š, å„ä¸ª node èŠ‚ç‚¹è´Ÿè½½å‡è¡¡.
- MostAllocated ç©ºé—²èµ„æºå°‘çš„åˆ†é«˜, ä¼˜å…ˆè°ƒåº¦åˆ°ç©ºé—²èµ„æºè¾ƒå°‘çš„ node ä¸Š, è¿™æ · pod å°½é‡é›†ä¸­èµ·æ¥æ–¹ä¾¿åé¢èµ„æºå›æ”¶.
- RequestedToCapacityRatio è¯·æ±‚ request å’Œ node èµ„æºæ€»é‡çš„æ¯”ç‡ä½çš„åˆ†é«˜.

```go
const (
	// é»˜è®¤ç­–ç•¥, ç©ºé—²èµ„æºå¤šçš„åˆ†é«˜, ä¼˜å…ˆè°ƒåº¦åˆ°ç©ºé—²èµ„æºå¤šçš„èŠ‚ç‚¹ä¸Š.
	LeastAllocated ScoringStrategyType = "LeastAllocated"

	// ç©ºé—²èµ„æºå°‘çš„åˆ†é«˜, ä¼˜å…ˆè°ƒåº¦åˆ°ç©ºé—²èµ„æºå°‘çš„ node ä¸Š
	MostAllocated ScoringStrategyType = "MostAllocated"

	// request å’Œ node æ€»é‡çš„æ¯”ç‡ä½çš„åˆ†é«˜
	RequestedToCapacityRatio ScoringStrategyType = "RequestedToCapacityRatio"
)

// ä¸‹é¢æ˜¯ NodeResourcesFit çš„å·¥å‚æ–¹æ³•, æ ¹æ® strategy ç­–ç•¥ä½¿ç”¨ä¸åŒçš„ scorer æ‰“åˆ†ç­–ç•¥.
func NewFit(plArgs runtime.Object, h framework.Handle, fts feature.Features) (framework.Plugin, error) {
	// è·å–ä¼ å…¥å‚æ•°
	args, ok := plArgs.(*config.NodeResourcesFitArgs)
	...

	strategy := args.ScoringStrategy.Type

	// æ ¹æ®ç­–ç•¥é€‰æ‹© score æ’ä»¶
	scorePlugin, exists := nodeResourceStrategyTypeMap[strategy]
	if !exists {
		return nil, fmt.Errorf("scoring strategy %s is not supported", strategy)
	}

	return &Fit{
		ignoredResources:         sets.NewString(args.IgnoredResources...),
		ignoredResourceGroups:    sets.NewString(args.IgnoredResourceGroups...),
		handle:                   h,
		resourceAllocationScorer: *scorePlugin(args),
	}, nil
}

// ä¸‹é¢å®šä¹‰äº†ä¸‰ä¸ª scorer æ‰“åˆ†ç­–ç•¥.
var nodeResourceStrategyTypeMap = map[config.ScoringStrategyType]scorer{
	config.LeastAllocated: func(args *config.NodeResourcesFitArgs) *resourceAllocationScorer {
		resources := args.ScoringStrategy.Resources
		return &resourceAllocationScorer{
			Name:      string(config.LeastAllocated),
			scorer:    leastResourceScorer(resources),
			resources: resources,
		}
	},
	config.MostAllocated: func(args *config.NodeResourcesFitArgs) *resourceAllocationScorer {
		resources := args.ScoringStrategy.Resources
		return &resourceAllocationScorer{
			Name:      string(config.MostAllocated),
			scorer:    mostResourceScorer(resources),
			resources: resources,
		}
	},
	config.RequestedToCapacityRatio: func(args *config.NodeResourcesFitArgs) *resourceAllocationScorer {
		resources := args.ScoringStrategy.Resources
		return &resourceAllocationScorer{
			Name:      string(config.RequestedToCapacityRatio),
			scorer:    requestedToCapacityRatioScorer(resources, args.ScoringStrategy.RequestedToCapacityRatio.Shape),
			resources: resources,
		}
	},
}
```

k8s scheduler å†…ç½®æ’ä»¶çš„ Filter è¿‡æ»¤æ–¹æ³•å¾ˆå¥½ç†è§£, å†³å®šèŠ‚ç‚¹æ˜¯å¦é€šè¿‡é¢„é€‰. è€Œ Score æ‰“åˆ†æ–¹æ³•åˆ™ä¸å¥½ç†è§£, ä½†é€šè¿‡æºç ä¸å¥½åæ¨æ‰“åˆ†çš„è®¡ç®—å…¬å¼æ˜¯æ€ä¹ˆæ¥çš„.

### NodeAffinity èŠ‚ç‚¹äº²å’Œæ€§æ’ä»¶åŸç†

NodeAffinity æ˜¯å®ç°èŠ‚ç‚¹äº²å’Œæ€§çš„æ’ä»¶, ä¸»è¦å®ç°äº† PreFilter, Filter å’Œ PreScore, Score å››ä¸ªæ–¹æ³•. PreFilter å’Œ PreScore åœ¨ NodeAffinity æ’ä»¶é‡Œåš state ä¼ é€’, è¿™é‡Œé‡ç‚¹åˆ†æ Filter å’Œ Score æ–¹æ³•å®ç°.

- `Filter` ç”¨æ¥åˆ¤æ–­ä¼ å…¥çš„ node æ˜¯å¦åŒ¹é… pod çš„ `RequiredDuringSchedulingIgnoredDuringExecution` ç¡¬äº²å’Œé…ç½®, ä¸é€‚é…åˆ™è¿”å›å¼‚å¸¸.

- `Score` ç”¨æ¥ç»™ node æ‰“åˆ†, å¦‚æœèŠ‚ç‚¹ labels é€‚é… pod çš„ `preferredDuringSchedulingIgnoredDuringExecution`, åˆ™æŠŠ spec.nodeAffinity.weight ç´¯åŠ åˆ° score.

é¦–å…ˆçœ‹ä¸‹ pod nodeAffinity èŠ‚ç‚¹äº²å’Œæ€§çš„ä¸¤ä¸ªå‚æ•°.

- `RequiredDuringSchedulingIgnoredDuringExecution` å‚æ•°è¡¨ç¤ºè°ƒåº¦å™¨åªä¼šè°ƒåº¦åˆ°ç¬¦åˆ pod è¦æ±‚çš„èŠ‚ç‚¹ä¸Š, æ²¡æœ‰ç¬¦åˆè¦æ±‚çš„èŠ‚ç‚¹åˆ™ä¸è¿›è¡Œè°ƒåº¦.

- `preferredDuringSchedulingIgnoredDuringExecution` å‚æ•°è¡¨ç¤ºè°ƒåº¦å™¨ä¼šä¼˜å…ˆå°è¯•å¯»æ‰¾æ»¡è¶³äº²å’Œæ€§è§„åˆ™çš„èŠ‚ç‚¹. å¦‚æœå®åœ¨æ‰¾ä¸åˆ°åŒ¹é…äº²å’Œæ€§çš„èŠ‚ç‚¹, è°ƒåº¦å™¨ä¼šé€‰æ‹©ä¸€ä¸ªåˆ†å€¼é«˜ä½†ä¸æ»¡è¶³ pod äº²å’Œæ€§è¦æ±‚çš„èŠ‚ç‚¹.

ä¸‹é¢æ˜¯åŒ…å« pod çš„äº²å’Œæ€§çš„ pod é…ç½®æ–‡ä»¶, å¯å¯¹ç…§è¯¥é…ç½®æ¥ç†è§£ nodeAffinity æ’ä»¶çš„ Filter å’Œ Score çš„å®ç°.

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: test-xiaorui-cc
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: kubernetes.io/os
            operator: In
            values:
            - linux
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 50
        preference:
          matchExpressions:
          - key: nodenames
            operator: In
            values:
            - xiaorui.cc
  containers:
  - name: with-node-affinity
    image: registry.k8s.io/pause:2.0
```

æºç ä½ç½®: `pkg/scheduler/framework/plugins/nodeaffinity/node_affinity.go`

```go
type NodeAffinity struct {
	handle              framework.Handle
	addedNodeSelector   *nodeaffinity.NodeSelector
	addedPrefSchedTerms *nodeaffinity.PreferredSchedulingTerms
}

var _ framework.FilterPlugin = &NodeAffinity{}
var _ framework.ScorePlugin = &NodeAffinity{}
...

// æ£€æŸ¥ä¼ å…¥çš„ node æ˜¯å¦åŒ¹é…è¯¥ pod çš„ nodeAffinity
func (pl *NodeAffinity) Filter(ctx context.Context, state *framework.CycleState, pod *v1.Pod, nodeInfo *framework.NodeInfo) *framework.Status {
	node := nodeInfo.Node()
	// node å¯¹è±¡ä¸ºç©ºåˆ™è·³å‡º
	if node == nil {
		return framework.NewStatus(framework.Error, "node not found")
	}

	...

	// è·å– prefilter é˜¶æ®µå†™å…¥çš„çŠ¶æ€, å…¶å®å°±æ˜¯ pod çš„ required é…ç½®.
	s, err := getPreFilterState(state)
	if err != nil {
		s = &preFilterState{requiredNodeSelectorAndAffinity: nodeaffinity.GetRequiredNodeAffinity(pod)}
	}

	// åˆ¤æ–­ pod çš„ required è·Ÿ node æ˜¯å¦é€‚é…
	match, _ := s.requiredNodeSelectorAndAffinity.Match(node)
	if !match {
		// å¦‚ä¸é€‚é…ç›´æ¥é€€å‡º
		return framework.NewStatus(framework.UnschedulableAndUnresolvable, ErrReasonPod)
	}

	return nil
}

// è·Ÿ pod å’Œ node äº²å’Œæƒ…å†µè¿›è¡Œæ‰“åˆ† score
func (pl *NodeAffinity) Score(ctx context.Context, state *framework.CycleState, pod *v1.Pod, nodeName string) (int64, *framework.Status) {
	...
	node := nodeInfo.Node()

	var count int64
	if pl.addedPrefSchedTerms != nil {
		count += pl.addedPrefSchedTerms.Score(node)
	}

	// è·å– prescore é˜¶æ®µå†™å…¥çš„ state
	s, err := getPreScoreState(state)
	if err != nil {
		// Fallback to calculate preferredNodeAffinity here when PreScore is disabled.
		preferredNodeAffinity, err := getPodPreferredNodeAffinity(pod)
		if err != nil {
			return 0, framework.AsStatus(err)
		}
		s = &preScoreState{
			preferredNodeAffinity: preferredNodeAffinity,
		}
	}

	// æ ¹æ® pod çš„ preferred å’Œ node labels çš„é€‚é…æƒ…å†µ, å¢åŠ  weight åˆ° score åˆ†å€¼.
	if s.preferredNodeAffinity != nil {
		count += s.preferredNodeAffinity.Score(node)
	}

	return count, nil
}

func (t *PreferredSchedulingTerms) Score(node *v1.Node) int64 {
	var score int64
	nodeLabels := labels.Set(node.Labels)
	nodeFields := extractNodeFields(node)
	for _, term := range t.terms {
		// å¦‚æœ node åŒ¹é… pod preferred, åˆ™å¢åŠ å®šä¹‰çš„æƒé‡.
		if ok, _ := term.match(nodeLabels, nodeFields); ok {
			score += int64(term.weight)
		}
	}
	return score
}
```

## æ€»ç»“

kubernetes scheduler ä» informer ç›‘å¬æ–°èµ„æºå˜åŠ¨, å½“æœ‰æ–° pod åˆ›å»ºæ—¶, scheduler éœ€è¦ä¸ºå…¶åˆ†é…ç»‘å®šä¸€ä¸ª node èŠ‚ç‚¹. åœ¨è°ƒåº¦ Pod æ—¶è¦ç»è¿‡ä¸¤ä¸ªé˜¶æ®µ, å³ è°ƒåº¦å‘¨æœŸ å’Œ ç»‘å®šå‘¨æœŸ. 

è°ƒåº¦å‘¨æœŸåˆ†ä¸ºé¢„é€‰é˜¶æ®µå’Œä¼˜é€‰é˜¶æ®µ.

- é¢„é€‰ (Predicates) æ˜¯é€šè¿‡æ’ä»¶çš„ Filter æ–¹æ³•è¿‡æ»¤å‡ºç¬¦åˆè¦æ±‚çš„ node èŠ‚ç‚¹
- ä¼˜é€‰ (Priorities) åˆ™å¯¹é¢„é€‰å‡ºæ¥çš„ node èŠ‚ç‚¹è¿›è¡Œæ‰“åˆ†æ’åº

ç»‘å®šå‘¨æœŸé»˜è®¤åªæœ‰ä¸€ä¸ªé»˜è®¤æ’ä»¶, ä»…ç»™ apiserver å‘èµ· node è·Ÿ pod ç»‘å®šçš„è¯·æ±‚.